<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Quantum Definitions</title>
    <link rel="stylesheet" href="style.css">
    <!-- allow latex, inline equations go between \(\) 
    and block equations go in \[\]-->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <div class="mainContent">
      <h1 id="top">Quantum Definitions</h1>
      <h2 id="toc">Table of Contents</h2>
        <div class="mainContent scrollBox" style="height:200px">
            <ul>
                <li><a href="#expectation">Expectation Value</a></li>
                <li><a href="#variance">Variance</a></li>
                <li><a href="#commutators">Commutators</a></li>
                <li><a href="#anticomm">Anti-commutators</a></li>
                <li><a href="#compatibleObs">Compatible Observables</a></li>
                <li><a href="#csco">Complete Sets of Commuting Observables</a></li>
                <li><a href="#incompatibleObs">Incompatible Observables</a></li>
                <li><a href="#heisenberg">Heisenberg Uncertainty Principle</a></li>
                <li><a href="#unitary">Unitary Operators</a></li>
                <li><a href="#poswavefcn">Position-Space Wave Functions</a></li>
                <li><a href="#momwavefcn">Momentum-Space Wave Functions</a></li>
                <li><a href="#ops">Position and Momentum Operators</a></li>
                <li><a href="#cnQuant">Canonical Quantizations</a></li>
            </ul>
        </div>
      <h2 id="expectation">Expectation Value</h2>
      <div class="mainContent">
        The expectation value of an observable \(A\) when the system is already in normalized state \(\mid\alpha\rangle\) is:
        \[\langle A\rangle_\alpha=\langle\alpha\mid A\mid\alpha\rangle\]
      </div>
      <p>
        <u>Note 1</u>: This definition matches the traditional defintion of expected value. 
        Say that we are in state \(\mid\alpha\rangle\), which is a combination of eigenvectors of \(A\), including \(\mid a'\rangle\) and \(\mid a''\rangle\) which correspond to eigenvalues \(a'\) and \(a''\):
        \[
          \begin{align}
            \langle A\rangle_\alpha &= \langle\alpha\mid A\mid\alpha\rangle \\
            &= \sum_{a''}\sum_{a'} \langle\alpha\mid a''\rangle\langle a''\mid A\mid a'\rangle\langle a'\mid\alpha\rangle \\
            &= \sum_{a'} \langle\alpha\mid a'\rangle\langle a'\mid A\mid a'\rangle\langle a'\mid\alpha\rangle \\
            &= \sum_{a'} |\langle\alpha\mid a'\rangle|^2\langle a'\mid A\mid a'\rangle\\
            &= \sum_{a'} |\langle\alpha\mid a'\rangle|^2 a' \\
          \end{align}
        \]
        Which is exactly the traditional definition of expected value.
        This is because the double summation has terms that are 0 whenever \(a''\neq a'\), and \(\langle a'\mid A\mid a'\rangle=a'\), since it is the expectation value of \(A\) when the system has already been measured and \(a'\) observed.
        <br><br>
        Clearly, if you measure \(A\) and observe \(a'\), the state of the system is in a corresponding eigenstate of \(A\). If you measure the same quantity immediately after, the system is still in this state and you will always observe \(a'\).
        <br><br>
        <u>Note 2</u>: You can also find this relationship in "reverse". Given that the probability of observing \(\lambda\) is \(p_\lambda\), we can start from the traditional definition of expected value and achieve the above definition from there:
        \[
          \begin{align}
            \langle A\rangle_\alpha &= \sum_{\lambda\in\sigma_A}\lambda p_\lambda \\
            &= \sum_{\lambda\in\sigma_A}\lambda \frac{||P_\lambda \mid\alpha\rangle||^2}{||\mid\alpha\rangle||^2} \\
            &= \sum_{\lambda\in\sigma_A}\lambda \frac{\langle P_\lambda \alpha\mid P_\lambda \alpha\rangle}{\langle\alpha\mid\alpha\rangle}
            = \sum_{\lambda\in\sigma_A}\lambda \frac{\langle \alpha P_\lambda\mid P_\lambda \alpha\rangle}{\langle\alpha\mid\alpha\rangle}
            = \sum_{\lambda\in\sigma_A}\lambda \frac{\langle \alpha\mid P_\lambda P_\lambda\mid \alpha\rangle}{\langle\alpha\mid\alpha\rangle} \\
            &= \sum_{\lambda\in\sigma_A}\lambda \frac{\langle \alpha\mid P_\lambda \mid \alpha\rangle}{\langle\alpha\mid\alpha\rangle} \\
            &= \frac{\langle \alpha\mid \sum_{\lambda\in\sigma_A}\lambda P_\lambda \mid \alpha\rangle}{\langle\alpha\mid\alpha\rangle} \\
            &= \frac{\langle \alpha\mid A \mid \alpha\rangle}{\langle\alpha\mid\alpha\rangle}
          \end{align}    
        \]
        This is the definition above, except we don't have to ensure that \(\mid\alpha\rangle\) is normalized.
      </p>
      <h2 id="variance">Variance</h2>
      <div class="mainContent">
        The variance (or dispersion, or mean-squared deviation) of an observable \(A\) is small or 0 when the state in question is an eigenstate of \(A\), so that measuring the system in this state has a well-defined value.
        We can think of it as the uncertainty in a measurement of \(\mathscr{A}\).
        <br><br>
        Mathematically, it is defined as: \(\langle(\Delta A)^2\rangle\) where \(\Delta A=A-\langle A \rangle\)
        This means that:
        \[
          \langle(\Delta A)^2\rangle = \langle (A^2 - 2A\langle A\rangle + \langle A\rangle^2)\rangle = \langle A^2\rangle - \langle A\rangle^2
        \]
        For <i>any</i> state we have the following inequality for observables \(A\) and \(B\):
        \[
          \langle(\Delta A)^2\rangle\langle(\Delta B)^2\rangle \geq \frac{1}{4}|\langle[A,B]\rangle|^2
        \]
        and the <i>standard deviation</i> is defined accordingly:
        \[
          \sigma_A=\sqrt{\langle(\Delta A)^2\rangle} = \sqrt{\langle A^2\rangle - \langle A\rangle^2}
        \]
      </div>
      <p>
        <u>Note</u>: To see the full proof of the correctness of the expression for standard deviation above, click <a href="variance.html">here</a>.
      </p>
      <h2 id="commutators">Commutators</h2>
      <div class="mainContent">
        The commutator of two observables, \(A\) and \(B\), have commutator: \[[A, B]=AB-BA\]
        They are said to <i>commute</i> if \([A,B]=0\), which means that the order in which you measure \(A\) and \(B\) doesn't affect the measurements, and that the probability of each measurement is unchanged.
      </div>
      <h2 id="anticomm">Anti-commutators</h2>
      <div class="mainContent">
        The anti-commutator of observables \(A\) and \(B\) is
        \[\{A,B\}=AB+BA\]
      </div>
      <h2 id="compatibleObs">Compatible Observables</h2>
      <div class="mainContent">
        From our <a href="qmPostulates.html">quantum postulates</a> we know that an observable is a self-adjoint operator in a Hilbert space \(H\).
        Two operators, \(A\) and \(B\) are compatible if:
        \[A=A^\dagger, B=B^\dagger, [A,B]=0\]
      </div>
      <p>
        <u>Note 1</u>: You can define a common basis of eigenvectors for compatible operators. Click <a href="commonBasis.html">here</a> to see the full demonstration.
        <br><br>
        <u>Note 2</u>: The order in which you measure compatible observables does <i>not</i> change what values are measured or the probabilities with which they are obtained:
        <br><br>
        Say we measure \(\mathscr{A}\), then \(\mathscr{B}\), and we know:
        \[
          [A,B]=0, A=\sum_{\lambda\in \sigma_A}\lambda P_\lambda, B=\sum_{\mu\in \sigma_B}\mu \Lambda_\mu
        \]
        Because \(A\) and \(B\) share a <a href="commonBasis.html">common basis of eigenvectors</a>, we can say that:
        \[
          \begin{align}
            P_{\lambda'}\mid\lambda, \mu, i\rangle &= \delta_{\lambda \lambda'}\mid\lambda, \mu, i\rangle \\
            \Lambda_{\mu'}\mid\lambda, \mu, i\rangle &= \delta_{\mu \mu'}\mid\lambda, \mu, i\rangle \\
            \Lambda_\mu P_{\lambda'}\mid\lambda, \mu, i\rangle &= \delta_{\mu \mu'}\delta_{\lambda \lambda'}\mid\lambda, \mu, i\rangle \\
            &= P_{\lambda'}\Lambda_\mu(\mid\lambda, \mu, i\rangle)
          \end{align}
        \]
        which clearly tells us that \([P_\lambda, \Lambda_\mu]=0\).
        <br><br>
        Then, if we we have a system in state \(\mid\psi\rangle\) and measure \(A\) and observe \(\lambda\) first, then we measure \(B\) and observe \(\mu\), the final state is \(\Lambda_\mu P_\lambda\mid\psi\rangle\).
        If we measure \(B\) and observe \(\mu\) first, then we measure \(A\) and observe \(\lambda\), the final state is \(P_\lambda\Lambda_\mu\mid\psi\rangle\).
        Obviously, the final state after doing the two measurements is the same regardless of the order.
        <br><br>
        This also means that the probabilities are the same:
        \[
          \begin{align}
            p_{\lambda,\mu} &= p_\lambda\cdot p_{\mu\mid\lambda} = \frac{||P_\lambda\mid\psi\rangle||^2}{||\mid\psi\rangle||^2}\cdot\frac{||\Lambda_\mu P_\lambda\mid\psi\rangle||^2}{||P_\lambda\mid\psi\rangle||^2} = \frac{||\Lambda_\mu P_\lambda\mid\psi\rangle||^2}{||\mid\psi\rangle||^2} \\
            p_{\mu,\lambda} &= p_\mu\cdot p_{\lambda\mid\mu} = \frac{||\Lambda_mu\mid\psi\rangle||^2}{||\mid\psi\rangle||^2}\cdot\frac{||P_\lambda\Lambda_\mu\mid\psi\rangle||^2}{||P_\lambda\mid\psi\rangle||^2} = \frac{||P_\lambda \Lambda_\mu \mid\psi\rangle||^2}{||\mid\psi\rangle||^2}
          \end{align} 
        \]
      </p>
      <h2 id="csco">Complete Sets of Commuting Observables</h2>
      <div class="mainContent">
        If we have two observables, \(A\) and \(B\) which are degenerate in the same eigenspaces, <i>we cannot uniquely describe a state with the observed values from \(A\) and \(B\) alone</i>.
        A complete set of commuting observables (CSCO) is a set of commuting observables whose measurements can uniquely describe a state.
        <br><br>
        In other words, their eigenvalues identify a unique vector in the space, and their common eigenvectors can be used as a basis to describe any state.
      </div>
      <p>
        <u>Notes</u>:
        <ul>
          <li>
            If we have an observable \(A\) which is not degenerate, each possible eigenvalue uniquely corresponds to one dimension on the space \(A\) operates in.
            When we observe \(mathscr{A}\), then, we know exactly what state the system is in, since each eigenvalue corresponds to one unique eigenvector.
            <br><br>
            Non-degenerate observables are in themselves CSCOs.
          </li>
          <br>
          <li>
            If we have an observable \(A\) that <i>is</i> degenerate, then a single measurement may not tell use exactly what state the system is in, since one eigenvalue then corresponds to more than one eigenvector.
            We need to distinguish between the eigenvectors corresponding to degenerate eigenvalues!
            <br><br>
            We can do this if we have an observable \(B\) which is <i>not</i> degenerate in the same spaces as \(A\), and commutes with \(A\).
            We know that there is a <a href="commonBasis.html">common basis of eigenvectors</a> between them.
            Any pair of eigenvalues from \(A\) and \(B\) will then correspond to different subsets of the space in which they operate, and as we know, the <a href="definitions.html#eigen">eigenspaces</a> of an observable do not overlap and cover the entire space they operate in.
            <br><br>
            This means that any pair of eigenvalues will uniquely isolate a specific eigenvector/dimension of the space!
          </li>
        </ul>
        <u>Examples</u>:
        <ul>
          <li>
            To uniquely describe a quantum state we need to specify: 
            <ul>
              <li>\(n\), the quantum energy level</li>
              <li>\(l\), the orbital angular momentum</li>
              <li>\(m\), the magnetic quantum number (which corresponds to the orientation of orbitals)</li>
              <li>\(\sigma\), the spin quantum number</li>
            </ul>
          </li>
          <li>
            Non-dgenerate observables like the Hamiltonian for the particle in a box configuration (knowing \(E_n=\frac{\hbar(n\pi)^2}{2m(b-a)^2}\) for box wall locations \(a, b\) and particle mass \(m\) is enough to describe the entire state) and \(S_z\) (knowing \(\pm\frac{\hbar}{2}\) is enough to tell us \(\mid\pm\rangle\) for sure) are CSCOs on their own.
          </li>
          <li>
            \(\{L^2,L_z\}\) is a CSCO: If our Hilbert space \(H=\{\frac{x^ny^mz^k}{\pi{n+m+k}}, n,m,k\in\mathbb{N}\}\), and 
            \[
              \langle f\mid g\rangle = \int_{-1}^1\int_0^{2\pi} d\cos\theta d\phi \cdot f^*(\theta,\phi)g(\theta,\phi)
            \]
            (see spherical harmonics), then \(L^2\) gives you \(l=0,1,2... \), where each \(l\gt 0\) has multiple corresponding states,
            and \(L_z\) gives you \(m=-l,...,l\) to differentiate between those states,
            or \(L^2\mid f\rangle = \hbar^2 l(l+1)\mid f\rangle\) and \(L_z\mid f\rangle=\hbar m \mid f\rangle\).
            So \(\mid f \rangle\) is unique described up to a phase factor/normalization factor.
          </li>
        </ul>
      </p>
      <h2 id="incompatibleObs">Incompatible Observables</h2>
      <div class="mainContent">
        Two observables, \(A\) and \(B\) are incompatible if \([A,B]\neq 0\)
      </div>
      <p>
        <u>Notes</u>: Measuring more than one observable will always change the final <i>state</i> of the system compared to <i>the resulting state when just a single observable is measured</i>, regardless of whether the observables commute or not.
        For a more in-depth discussion click <a href="changedState.html">here</a>.
      </p>
      <h2 id="heisenberg">Heisenberg Uncertainty Principle</h2>
      <div class="mainContent">
        This is a theorem that states that for any two observables \(A\) and \(B\):
        \[\sigma_a\sigma_b \geq \frac{1}{2}|\langle[A,B]\rangle|\]
      </div>
      <p>
        <u>Note 1</u>: For the derivation of this theorem, click <a href="heisenberg.html">here</a>.
        <br><br>
        <u>Note 2</u>: This was briefly discussed when we talked about <a href="variance.html">standard deviation</a>, but if two observables \(A\) and \(B\) are compatible, the uncertainty of measuring a system in an eigenstate of \(A\) and \(B\) is 0.
        <br><br>
        This is not guaranteed for non-compatible observables, but it is <i>possible</i> to find <i>some</i> (but obviously not all) shared eigenstates where measuring both quantities has an uncertainty of 0.
        For example:
        <br><br>
        Momentum and position famously do not commute: \([x, p]=i\hbar\), which tells us that \(\sigma_A\sigma_B \geq \frac{1}{2}|\langle\psi \mid i\hbar\mid\psi\rangle|\), and \(\frac{1}{2}|\langle\psi \mid i\hbar\mid\psi\rangle|\) is the constant \(\frac{\hbar}{2}\), which means that neither \(\sigma_A\) nor \(\sigma_B\) can ever get arbitrarily small enough to make the left-hand side of the inequality 0, their product is always a non-zero value.
        <br><br>
        Angular momentum observables \(L_x\) and \(L_y\) also do not commute; \([L_x,L_y]=i\hbar L_z\), so the value on the right-hand side of our inequality <i>can</i> be 0, provided that \(L_z=0\).
        Indeed, \(\mid l,m\rangle=\mid0,0\rangle\) yields \(\sigma_{L_x}=\sigma_{L_y}=0\), but you can't do this for <i>all</i> eigenstates of these two observables.
      </p>
      <h2 id="unitary">Unitary Operators</h2>
      <div class="mainContent">
        An operator \(U\) is unitary if \(UU^\dagger=U^\dagger U=\mathbb{I}\).
        \(U\) must be invertible and \(U^{-1}=U^\dagger\).
        <br><br>
        Unitary operators are surjective isometries, and preserve the scalar product/norm.
        <br><br>
        \([U, U^\dagger]=0\), and \(U\) has <i>complex</i> eigenvalues.
      </div>
      <p>
        <u>Note</u>: For a more in-depth discussion of these properties, see <a href="unitary.html">Features of Unitary Transformations</a>.
      </p>
      <h2 id="poswavefcn">Position-Space Wave Functions</h2>
      <div class="mainContent">
        A <i>position-space</i> wave function describes a system's position in time. 
        <br><br>
        Since a system can be in a superposition of multiple states, we have, in the past, written the state of a system as \(|\alpha\rangle=\sum_i c_i|i\rangle, c_i=\langle i|\alpha\rangle\).
        Now we have a continuous set of possible positions, so we write 
        \[|\alpha\rangle=\int dx |x\rangle\langle x|\alpha\rangle\tag{1}\]
        instead, where \(|x\rangle\) are eigenstates of position operator \(\hat{x}\).
        (In a dynamic system, \(|\alpha\rangle\) will evolve with time!)
        <br><br>
        The expression \(\langle x |\alpha\rangle\) is called the <i>position-space</i> wave function (or just wave function) for a specific state \(|\alpha\rangle\), and is often written as \(\psi_\alpha(x)\).
        <br><br>
        From (1) we can see that the <i>probability density</i> of finding the system at position \(x\) at time \(t\) is:
        \[|\langle x|\alpha\rangle|^2 dx\]
        for normalized \(|\alpha\rangle\) 
        (else we would have to normalize ourselves. Recall Born's Rule from the <a href="qmPostulates.html">quantum postulates</a>).
        That is, it is the probability that the system is at a position within a distance \(dx\) of position \(x\).
        Clearly then, for normalized \(|\alpha\rangle\),
        \[\int|\langle x|\alpha\rangle|^2 dx = \int \langle\alpha|x\rangle\langle x|\alpha\rangle dx = \hat{1}\langle\alpha|\alpha\rangle = 1\]
      </div>
      <p>
        <u>Notes</u>:
          <ul>
            <li>When we say \(\langle x|\alpha\rangle\), \(x\) is <i>not</i> a specific value, but a variable, and the state \(|\alpha\rangle\) evolves over time, so \(\langle x|\alpha\rangle\) is not a scalar value even if \(x\) is fixed, it's a vector.</li>
            <li>
              Say we have two states \(|\alpha\rangle\) and \(|\beta\rangle\), and wave functions \(\psi_\alpha(x)=\langle x|\alpha\rangle\) and \(\psi_\beta(x)=\langle x|\beta\rangle\).
              The <i>probability amplitude for a system in state \(|\alpha\rangle\) to be found in state \(|\beta\rangle\)</i> is:
              \[
                \langle\beta|\alpha\rangle
                =\int dx\langle\beta|x\rangle\langle x|\alpha\rangle
                =\int dx \psi_\beta^*(x)\psi_\alpha(x)
              \]
            </li>
          </ul>
      </p>
      <h2 id="momwavefcn">Momentum-Space Wave Functions</h2>
      <div class="mainContent">
        As before, the state of a system \(|\alpha\rangle\) can be written as:
        \[|\alpha\rangle=\int dp |p\rangle\langle p|\alpha\rangle\tag{1}\]
        where \(|p\rangle\) are eigenstates of position operator \(\hat{p}\).
        Instead of \(\psi\), \(\phi\) is used for the momentum-space wave function, and \(\langle p|\alpha\rangle\) can be written as \(\phi_\alpha(p)\).
        <br><br>
        Again, from (1) we can see that the <i>probability density</i> of observing that the system has momentum \(p\) at time \(t\) is:
        \[|\langle p|\alpha\rangle|^2 dp\]
        for normalized \(|\alpha\rangle\).
        Then, for normalized \(|\alpha\rangle\),
        \[\int|\langle p|\alpha\rangle|^2 dp = 1\]
      </div>
      <p>
        <u>Notes</u>: 
        <ul>
          <li>
            For wave functions in general, since the first argument (the \(b\) in \(\langle b| a\rangle)\) is a variable that can be set to a fixed value, and because we can also think of this as projecting \(|a\rangle\) along a specific \(|b\rangle\), 
            when we operate on a wave function with some operator \(\hat O\), we're really doing \(\langle b|\hat O|a\rangle\).
            <br><br>
            In other words, \(\hat O \psi_\alpha(x) = \langle x|\hat O|\alpha\rangle\) and \(\hat O \phi_\alpha(p) = \langle p|\hat O|\alpha\rangle\)
          </li>
          <br>
          <li>If we know what the state of a system is, sometimes we just say \(\psi(x)\) and \(\phi(p)\) instead of \(\psi_\alpha(x)\) and \(\phi_\alpha(p)\).</li>
        </ul>
      </p>
      <h2 id="ops">Position and Momentum Operators</h2>
      <div class="mainContent">
        Say we have a system in state \(|\alpha\rangle\). 
        In the position basis: 
        \[\hat{x}\psi(x) = x\psi(x) = x\langle x|\alpha\rangle\] 
        and 
        \[\hat{p}\psi(x) = -i\hbar\frac{d}{dx}\psi(x) = -i\hbar\frac{d}{dx}\langle x|\alpha\rangle\]
        (see demonstration <a href="classical-qm.html#phat">here</a>)
        where measuring momentum of a system with position-space wave function \(\psi(x)\) projects the system into an eigenstate of momentum, and yields another function that varies with \(x\).
        Essentially this tells us that a single momentum eigenstate corresponds with many position eigenstates.
        <br><br>
        Finally, of course,
        \[[\hat{x},\hat{p}]=i\hbar\] (see demonstration <a href="classical-qm.html#xpcomm">here</a>)
        <hr>
        The "transformation function" \(\langle x|p\rangle\) is:
        \[\langle x|p\rangle = \phi_p(x) = \frac{1}{\sqrt{2\pi\hbar}} e^{ipx/\hbar}\]
        See demonstration <a href="dynamics.html#momStateinPosBasis">here</a>.
        <hr>
        If we want to change bases from position-space to momentum-space, we can use:
        \[\begin{align}
          \psi_\alpha(x) &= \frac{1}{\sqrt{2\pi\hbar}} \int dp \text{ } exp[\frac{ipx}{\hbar}] \phi_\alpha(p) \\
          \phi_\alpha(x) &= \frac{1}{\sqrt{2\pi\hbar}} \int dx \text{ } exp[\frac{-ipx}{\hbar}] \psi_\alpha(p)
        \end{align}\]
        or, in three dimensions:
        \[\begin{align}
          \psi_\alpha(\vec{x}) &= \frac{1}{2\pi\hbar^{3/2}} \int d^3p \text{ } exp[\frac{i\vec{p}\cdot\vec{x}}{\hbar}] \phi_\alpha(\vec{p}) \\
          \phi_\alpha(\vec{p}) &= \frac{1}{\sqrt{2\pi\hbar}} \int d^3x \text{ } exp[\frac{-i\vec{p}\cdot\vec{x}}{\hbar}] \psi_\alpha(\vec{x})
        \end{align}\]
        where \(\vec{x}\) and \(\vec{p}\) are vectors \((x_i, x_j, x_k)\) and \((p_i, p_j, p_k)\) respectively.
      </div>
      <p>
        <u>Note</u>:
        To derive the change of basis equations above, we can use a matrix/operator just like we do for two arbitrary bases.
        <br><br>
        <!-- the transformation function \(\langle x|p\rangle\) is the probability amplitude of a system in momentum eigenstate \(|p\rangle\) being found at position \(x\).
        <br><br> -->
        <!-- What is \(\langle x|p\rangle\)? 
        Say that we are in state \(|p\rangle\), so our position-space wave function is \(\langle x|p \rangle\).
        Then:
        \[\begin{align}
          \hat{p}\langle x|p\rangle &= -i\hbar\frac{d}{dx}\langle x|p\rangle \\
          \hat{p}\langle x|p\rangle &= \langle x|p|p\rangle = p\langle x|p\rangle \\
          p\langle x|p\rangle &= -i\hbar \frac{d}{dx}\langle x|p\rangle
        \end{align}\]
        where the solution to this final line is:
        \[\langle x|p\rangle = N e^{\frac{ipx}{\hbar}}\]
        where \(N\) is a normalization constant \(\frac{1}{\sqrt{2\pi\hbar}}\).
        With this information in hand,  -->
        We can say that for an arbitrary state \(|\alpha\rangle\):
        \[\begin{align}
          \psi_\alpha(x) = \langle x|\alpha\rangle &= \int dp\langle x|p\rangle\langle p|\alpha\rangle \\
          &= \int dp\langle x|p\rangle \phi_\alpha(p) \\
          &= \int dp \frac{1}{\sqrt{2\pi\hbar}}e^{\frac{ipx}{\hbar}} \phi_\alpha(p) \\
          \psi_\alpha(x) &= \frac{1}{\sqrt{2\pi\hbar}} \int dp \text{ } e^{\frac{ipx}{\hbar}} \phi_\alpha(p) \\
        \end{align}\]
        and
        \[\begin{align}
          \phi_\alpha(p) = \langle p|\alpha\rangle &= \int dx\langle p|x\rangle\langle x|\alpha\rangle \\
          &= \int dx \langle x|p\rangle \psi_\alpha(p) \\
          &= \int dx \frac{1}{\sqrt{2\pi\hbar}}e^{-\frac{ipx}{\hbar}} \psi_\alpha(p) \\
          \phi_\alpha(x) &= \frac{1}{\sqrt{2\pi\hbar}} \int dx \text{ } e^{\frac{-ipx}{\hbar}} \psi_\alpha(p) \\
        \end{align}\]
      </p>
      <h2 id="cnQuant">Canonical Quantizations</h2>
      <div class="mainContent">
          Generically, a canonical classical transformation \(U_{d\alpha}\) generated by some \(G\) has a quantized counterpart generated by \(\hat G\):
                \[\hat{U}_{d\alpha}=\hat{1}-id\alpha\frac{\hat{G}}{\hbar} + o(d\alpha^2)\]
                \[\hat U_{d\alpha}|\psi(\alpha)\rangle = |\psi(\alpha+d\alpha\rangle)\]
                <hr>
                Time Translations are generated by \(\hat H\) and act as:
                \[\hat{U}_{\Delta t} = e^{- i\Delta t\frac{\hat{H}}{\hbar}} = \hat{1} - i\Delta t\frac{\hat{H}}{\hbar} + o(\Delta t^2) \]    
                \[\hat{U}_{\Delta t} |\psi(t)\rangle = |\psi(t + \Delta t)\rangle\]
                Space Translations are generated by \(\hat p\) and act as:
                \[\hat{T}_{\Delta x} = e^(i\Delta x\frac{\hat{p}}{\hbar}) = \hat{1} - i\Delta x\frac{\hat{p}}{\hbar} + o(\Delta x^2) \]
                \[\hat{T}_{\Delta x} |\psi(x)\rangle = |\psi(x + \Delta x)\rangle\]
                Momentum Boosts are generated by \(-\hat x\) and act as:
                \[\hat{B}_{\Delta p} = e^{i\Delta p\frac{\hat{x}}{\hbar}} = \hat{1} + i\Delta p\frac{\hat{x}}{\hbar} + o(\Delta p^2) \]    
                \[\hat{B}_{\Delta p} |\psi(p)\rangle = |\psi(p + \Delta p)\rangle\]
      </div>
      <p>
        <u>Notes</u>: To see how these <a href="classical.html#cts">classical canonical transformations</a> are quantized (brought into quantum mechanics) see the <a href="classical-qm.html">bridge between classical and quantum mechanics</a>.

      </p>
    </div>
    <footer class="fixed-footer">
      <a href="#top">↑</a>
  </footer>
  </body>
</html>